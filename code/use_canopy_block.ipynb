{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import time\n",
    "from collections import Counter\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import ast\n",
    "import networkx as nx\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Optional\n",
    "import builtins\n",
    "from lshashpy3 import LSHash\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"http_proxy\"] = \"http://localhost:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://localhost:7890\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"your api key \"\n",
    ")\n",
    "\n",
    "pre_prompt = (\"Please classify the following records into a two-dimensional list. Each element of the array \"\n",
    "              \"should be a group, containing the record IDs of that group (e.g., 1, 2, 3, etc.). Ensure that \"\n",
    "              \"each record ID is classified exactly once and appear once in the 2D array, without any \"\n",
    "              \"duplication or omission.The output should be a two-dimensional list with no additional information!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def normalize_and_clean(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return ''  \n",
    "    text = text.lower()  \n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  \n",
    "    return text\n",
    "\n",
    "def read_csv_and_clean_with_ids(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    if df.empty or df.shape[1] < 2:\n",
    "        raise ValueError(\"CSV is none \")\n",
    "    ids = df.iloc[:, 0]\n",
    "\n",
    "    data = df.iloc[:, 1:]\n",
    "\n",
    "    data.replace(\"\", np.nan, inplace=True)\n",
    "    data.fillna(\"\", inplace=True)\n",
    "    cleaned_data = []\n",
    "    for id_, row in zip(ids, data.values):\n",
    "        cleaned_row = [normalize_and_clean(str(item)) for item in row if normalize_and_clean(str(item))]\n",
    "        cleaned_data.append((int(id_), cleaned_row))  \n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "def build_inverted_index(cleaned_data):\n",
    "    inverted_index = defaultdict(set)  \n",
    "    for record_id, fields in cleaned_data:\n",
    "        for field in fields:\n",
    "            if field: \n",
    "                inverted_index[field].add(record_id)\n",
    "    return inverted_index\n",
    "\n",
    "def calculate_similarity_matrix(cleaned_data, inverted_index):\n",
    "    record_ids = [record_id for record_id, _ in cleaned_data]\n",
    "    record_sets = {record_id: set(fields) for record_id, fields in cleaned_data}\n",
    "    n = len(record_ids)\n",
    "    similarity_matrix = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                similarity_matrix[i][j] = 1.0  \n",
    "            else:\n",
    "                set_i = record_sets[record_ids[i]]\n",
    "                set_j = record_sets[record_ids[j]]\n",
    "\n",
    "                intersection = len(set_i & set_j)\n",
    "                union = len(set_i | set_j)\n",
    "\n",
    "                similarity_matrix[i][j] = intersection / union if union > 0 else 0.0\n",
    "\n",
    "    return similarity_matrix, record_ids\n",
    "\n",
    "def canopy_clustering(similarity_matrix, T1, T2):\n",
    "    \n",
    "    if T1 <= T2:\n",
    "        raise ValueError(\"T1 must larger than T2\")\n",
    "    if similarity_matrix.shape[0] != similarity_matrix.shape[1]:\n",
    "        raise ValueError(\"wrong!\")\n",
    "\n",
    "\n",
    "    n = similarity_matrix.shape[0]\n",
    "    unassigned_points = set(range(n))  \n",
    "    canopies = []  \n",
    "    block_T1 = []  \n",
    "\n",
    "    while unassigned_points:\n",
    "        center = unassigned_points.pop()\n",
    "        candidate_points = [\n",
    "            idx for idx in unassigned_points if similarity_matrix[center, idx] >= T2\n",
    "        ]\n",
    "        candidate_points.append(center)  \n",
    "        current_canopy = set(candidate_points)\n",
    "        strictly_removed = [\n",
    "            idx for idx in candidate_points if similarity_matrix[center, idx] >= T1\n",
    "        ]\n",
    "        tmp = []\n",
    "        for idx in strictly_removed:\n",
    "            unassigned_points.discard(idx) \n",
    "            tmp.append(idx)\n",
    "            current_canopy.discard(idx)  \n",
    "            \n",
    "        block_T1.append(tmp)\n",
    "        canopies.append(list(current_canopy))\n",
    "\n",
    "    return canopies, block_T1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ground_truth(file_path):\n",
    "    class UnionFind:\n",
    "            def __init__(self):\n",
    "                self.parent = {}\n",
    "\n",
    "            def find(self, x):\n",
    "                if self.parent[x] != x:\n",
    "                    self.parent[x] = self.find(self.parent[x])\n",
    "                return self.parent[x]\n",
    "\n",
    "            def union(self, x, y):\n",
    "                rootX = self.find(x)\n",
    "                rootY = self.find(y)\n",
    "                if rootX != rootY:\n",
    "                    self.parent[rootY] = rootX\n",
    "\n",
    "            def add(self, x):\n",
    "                if x not in self.parent:\n",
    "                    self.parent[x] = x\n",
    "\n",
    "    def merge_coordinates(coordinates):\n",
    "            uf = UnionFind()\n",
    "            ids = set()\n",
    "\n",
    "            for ltable_id, rtable_id in coordinates:\n",
    "                uf.add(ltable_id)\n",
    "                uf.add(rtable_id)\n",
    "                uf.union(ltable_id, rtable_id)\n",
    "                ids.add(ltable_id)\n",
    "                ids.add(rtable_id)\n",
    "\n",
    "            entity_groups = {}\n",
    "            for _id in ids:\n",
    "                root = uf.find(_id)\n",
    "                if root not in entity_groups:\n",
    "                    entity_groups[root] = []\n",
    "                entity_groups[root].append(_id)\n",
    "\n",
    "            result_1 = []\n",
    "            for root, records in entity_groups.items():\n",
    "                result_1.append(records)\n",
    "\n",
    "            return result_1\n",
    "    data = []\n",
    "    with open(file_path, newline='', encoding='MacRoman') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        next(reader)  \n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "\n",
    "    ground_truth_1 = merge_coordinates(data)\n",
    "    print(len(ground_truth_1))\n",
    "    ground_truth_new = []\n",
    "    for row in ground_truth_1:\n",
    "        tmp = []\n",
    "        for ids in row:\n",
    "            tmp.append(int(ids))\n",
    "        ground_truth_new.append(tmp)\n",
    "    return ground_truth_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(id_list, file_path):\n",
    "    lines = []\n",
    "    with open(file_path, 'r', encoding='MacRoman') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        rows = list(reader)\n",
    "        for r_id in id_list:\n",
    "            for row in rows:\n",
    "                if row['ID'] == str(r_id): \n",
    "                    lines.append(','.join([str(row[key]) for key in reader.fieldnames if key != 'ID']))\n",
    "                    break\n",
    "    prompts = '\\n'.join(lines)\n",
    "    return prompts\n",
    "\n",
    "def vectorize_data(text):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = model.encode(text.split('\\n')) \n",
    "    return embeddings\n",
    "\n",
    "def elbow_method(embeddings, max_k=5):\n",
    "\n",
    "    if embeddings is None or embeddings.shape[0] < 2:\n",
    "        return 1  \n",
    "    \n",
    "    distortions = []\n",
    "    K = range(1, min(max_k, embeddings.shape[0]) + 1)\n",
    "    \n",
    "    for k in K:\n",
    "        kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)\n",
    "        kmeans.fit(embeddings)\n",
    "        distortions.append(kmeans.inertia_)\n",
    "    \n",
    "    optimal_k_index = np.argmin(distortions[1:]) + 1  \n",
    "    optimal_k = K[optimal_k_index]\n",
    "\n",
    "    print(f\"best is : {optimal_k}\")\n",
    "    return optimal_k\n",
    "\n",
    "def kmeans_clustering(embeddings, n_clusters):\n",
    "\n",
    "    if embeddings is None or len(embeddings) < n_clusters:\n",
    "        return np.zeros(len(embeddings), dtype=int)  \n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, init='k-means++', random_state=42,n_init=10)\n",
    "    kmeans.fit(embeddings)\n",
    "    return kmeans.labels_\n",
    "\n",
    "def format_output(id_list, labels):\n",
    "    clusters = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        if label not in clusters:\n",
    "            clusters[label] = []\n",
    "        clusters[label].append(id_list[i])\n",
    "    \n",
    "    sorted_clusters = sorted(clusters.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "    \n",
    "    output = []\n",
    "    for cluster in sorted_clusters:\n",
    "        output.append([int(item) for item in cluster[1]])\n",
    "    \n",
    "    return output\n",
    "\n",
    "def read_csv_to_2d_array(file_path):\n",
    "    with open(file_path, 'r', encoding='MacRoman') as file:\n",
    "        reader = csv.reader(file)\n",
    "        data = list(reader)\n",
    "    return data\n",
    "\n",
    "def get_prompt_from_ids(id_list, file_path):\n",
    "    lines = []\n",
    "    with open(file_path, 'r', encoding='MacRoman') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        rows = list(reader)\n",
    "        for r_id in id_list:\n",
    "            for row in rows:\n",
    "                if row['ID'] == str(r_id): \n",
    "                    rec_str = f\"Record {r_id}: \"\n",
    "                    rec_str += ','.join([str(row[key]) for key in reader.fieldnames if key != 'ID'])\n",
    "                    lines.append(rec_str)\n",
    "                    break\n",
    "    return '\\n'.join(lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_sampling(original_array):\n",
    "    result = []\n",
    "    row_indices = {i: row.copy() for i, row in enumerate(original_array)}\n",
    "    total_ids = sum(len(row) for row in original_array)\n",
    "    \n",
    "    while True:\n",
    "        group = []\n",
    "        while len(group) < 10:\n",
    "            flag = False\n",
    "            for i in range(len(original_array)):\n",
    "                if row_indices[i]:  \n",
    "                    group.append(row_indices[i].pop(0))\n",
    "                    flag = True\n",
    "                if len(group) == 10: \n",
    "                    break\n",
    "            if not flag: \n",
    "                break\n",
    "        if group:\n",
    "            result.append(group)\n",
    "        if not any(row_indices.values()):  \n",
    "            break\n",
    "    output_ids = [id for group in result for id in group]\n",
    "    if len(output_ids) != total_ids:\n",
    "        raise ValueError(\"ID is not match ！\")\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_sampled_ids(csv_file,sample_ids_list):\n",
    "\n",
    "    execution_time=0\n",
    "    use_number = 0\n",
    "    total_tokens_call = 0\n",
    "    \n",
    "    all_classified_results = []\n",
    "    for ids in sample_ids_list:\n",
    "        content_prompt = get_prompt_from_ids(id_list = ids, file_path = csv_file)\n",
    "        start_time = time.time()\n",
    "        completion = client.chat.completions.create(\n",
    "                            model = \"gpt-4o-mini\",\n",
    "                            messages=[\n",
    "                            {\"role\": \"system\", \"content\": \"You are a worker with rich experience performing Entity Resolution tasks. You specialize in clustering and classification within ER.\"},\n",
    "                            {\"role\": \"user\", \"content\": pre_prompt + content_prompt},\n",
    "                            ]\n",
    "                        )\n",
    "        execution_time += (time.time() - start_time)\n",
    "        use_number += 1\n",
    "        token_number = completion.usage.total_tokens\n",
    "        total_tokens_call += token_number\n",
    "        content = completion.choices[0].message.content\n",
    "        content = content.replace('\\n', '').replace(' ', '')\n",
    "        content_cleaned = re.sub(r\"[^\\d\\[\\],]\", \"\", content)\n",
    "        content_cleaned = re.sub(r\",\\s*]\", \"]\", content_cleaned)\n",
    "        content_cleaned = re.sub(r\",+\", \",\", content_cleaned)\n",
    "        matches = re.findall(r'\\[([^\\[\\]]*?)\\]', content_cleaned)\n",
    "        result_llm = []\n",
    "        for match in matches:\n",
    "            match_cleaned = match.strip()\n",
    "            if ',' in match_cleaned:\n",
    "                sublist = [int(num) for num in match_cleaned.split(',')]\n",
    "                result_llm.append(sublist)\n",
    "            else:\n",
    "                result_llm.append([int(num) for num in match_cleaned.split()])\n",
    "        all_classified_results.append(result_llm) \n",
    "    return all_classified_results,execution_time,use_number,total_tokens_call\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def the_most_importent_one(vector_data,classified_results):\n",
    "\n",
    "    result_for_find = []\n",
    "    for classified_results_row in classified_results:\n",
    "        list_select = []\n",
    "        vectored_select = []\n",
    "        for cluster_row in classified_results_row:\n",
    "            vectored_select = np.array([vector_data[id_] for id_ in cluster_row])\n",
    "            avg_vector = np.mean(vectored_select, axis=0) \n",
    "            distances = [np.linalg.norm(vectored_select[i] - avg_vector) for i in range(len(cluster_row))]\n",
    "            representative_id = cluster_row[np.argmin(distances)]\n",
    "            list_select.append(representative_id)\n",
    "        result_for_find.append(list_select)\n",
    "    return result_for_find\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_2d_array_from_file(file_path):\n",
    "\n",
    "    array_list = []\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                row = list(map(int, line.strip().split()))\n",
    "                array_list.append(row)\n",
    "        return array_list\n",
    "    except FileNotFoundError:\n",
    "        print(f\" {file_path}  is not found\")\n",
    "    except ValueError as e:\n",
    "        print(f\"can not found：{e}\")\n",
    "    return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def the_most_importent_one_1(classified_results):\n",
    "\n",
    "    result_for_find = []\n",
    "    for classified_results_row in classified_results:\n",
    "        list_select = []\n",
    "        for cluster_row in classified_results_row:   \n",
    "            representative_id = cluster_row[0]\n",
    "            list_select.append(representative_id)\n",
    "        result_for_find.append(list_select)\n",
    "    return result_for_find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "def find_most_similar(\n",
    "    current_id: Optional[int], \n",
    "    candidate_ids: List[int], \n",
    "    similarity_matrix: List[List[float]]\n",
    ") -> Optional[int]:\n",
    "   \n",
    "    if not candidate_ids:\n",
    "        return None\n",
    "    \n",
    "    if current_id is None:\n",
    "        return candidate_ids[0]\n",
    "\n",
    "    max_similarity = -float('inf')\n",
    "    most_similar_id = None\n",
    "    \n",
    "    for candidate_id in candidate_ids:\n",
    "        similarity = similarity_matrix[current_id][candidate_id]\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            most_similar_id = candidate_id\n",
    "    \n",
    "    return most_similar_id\n",
    "\n",
    "def filter_available_ids(next_row_ids: List[int], id_assigned: set) -> List[int]:\n",
    "    \n",
    "    return [id_ for id_ in next_row_ids if id_ not in id_assigned]\n",
    "\n",
    "def process_rounds(\n",
    "    id_matrix: List[List[int]],\n",
    "    similarity_matrix: List[List[float]],\n",
    "    max_length: int\n",
    ") -> List[List[int]]:\n",
    "    \n",
    "    all_rounds = []\n",
    "    id_assigned = set()\n",
    "    total_rows = len(id_matrix)\n",
    "\n",
    "    while True:\n",
    "        current_round = []\n",
    "        current_id = None\n",
    "\n",
    "        for row_index in range(total_rows):\n",
    "            available_ids = filter_available_ids(id_matrix[row_index], id_assigned)\n",
    "            if not available_ids:\n",
    "                continue\n",
    "\n",
    "            next_id = find_most_similar(current_id, available_ids, similarity_matrix)\n",
    "            if next_id is not None:\n",
    "                current_round.append(next_id)\n",
    "                id_assigned.add(next_id)\n",
    "                current_id = next_id\n",
    "\n",
    "       \n",
    "\n",
    "        if not current_round:\n",
    "            break\n",
    "        \n",
    "        all_rounds.append(current_round)\n",
    "    \n",
    "    return all_rounds\n",
    "\n",
    "\n",
    "\n",
    "def traverse_ids_to_2d(\n",
    "    id_matrix: List[List[int]], \n",
    "    similarity_matrix: List[List[float]],\n",
    "    max_length: int = 10, \n",
    "    batch_size: int = 10\n",
    ") -> List[List[int]]:\n",
    "\n",
    "    all_rounds = process_rounds(id_matrix, similarity_matrix, max_length)\n",
    "    return all_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_find(x, pa):\n",
    "    if pa[x] != x:\n",
    "        pa[x] = merge_find(pa[x], pa)  \n",
    "    return pa[x]\n",
    "\n",
    "\n",
    "def merge_union(x, y, pa):\n",
    "    rootX = merge_find(x, pa)\n",
    "    rootY = merge_find(y, pa)\n",
    "    if rootX != rootY:\n",
    "        pa[rootY] = rootX  \n",
    "\n",
    "\n",
    "def find_simi_nex(small_clusters, now_cluster , pa , ini_simi,the_max_nex):\n",
    "    global i1, i2\n",
    "\n",
    "    pattern = []  \n",
    "    for i in range(len(now_cluster) - 1):\n",
    "        for j in range(i + 1, len(now_cluster)):\n",
    "            if ini_simi[now_cluster[i]][now_cluster[j]] >= the_max_nex:\n",
    "                # print(maper[now_cluster[i]], maper[now_cluster[j]])\n",
    "                # print([now_cluster[i], now_cluster[j]])\n",
    "                pattern.append([now_cluster[i], now_cluster[j]])\n",
    "    for x, y in pattern:\n",
    "        for i in range(len(small_clusters)):\n",
    "            if x in small_clusters[i]:\n",
    "                i1 = i\n",
    "                break\n",
    "        for i in range(len(small_clusters)):\n",
    "            if y in small_clusters[i]:\n",
    "                i2 = i\n",
    "                break\n",
    "        merge_union(i1, i2, pa)\n",
    "    merged_groups = {}\n",
    "    for i in range(len(small_clusters)):\n",
    "        root = merge_find(i, pa)\n",
    "        if root not in merged_groups:\n",
    "            merged_groups[root] = []\n",
    "        merged_groups[root].extend(small_clusters[i])\n",
    "    result = [sorted(set(values)) for values in merged_groups.values()]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def llm_seperate(data_list , data_file,ini_simi,the_max_nex):\n",
    "    api_call_time = 0\n",
    "    use_time = 0\n",
    "    use_token = 0\n",
    "    seperate_input_token = 0\n",
    "    seperate_output_token = 0\n",
    "    result_sliced = []\n",
    "    number = math.ceil(len(data_list)/10)\n",
    "    sliced_lists = [data_list[i * 10:(i + 1) * 10] for i in range(number)]\n",
    "    for one_slice in sliced_lists:\n",
    "        api_call_time+=1\n",
    "        start_time = time.time()\n",
    "        prompt_sliced = get_prompt_from_ids(one_slice, data_file)\n",
    "        completion = client.chat.completions.create(\n",
    "                        model=\"gpt-4o-mini\",\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\",\n",
    "                            \"content\": \"You are a worker specialize in clustering and classification within Entity Resolution.\"},\n",
    "                            {\"role\": \"user\", \"content\": pre_prompt + prompt_sliced},\n",
    "                        ]\n",
    "                    )\n",
    "        use_time += time.time() - start_time\n",
    "        prompt_tokens = completion.usage.prompt_tokens  \n",
    "        seperate_input_token += prompt_tokens\n",
    "        completion_tokens = completion.usage.completion_tokens  \n",
    "        seperate_output_token += completion_tokens\n",
    "        token_number = completion.usage.total_tokens\n",
    "        use_token += token_number\n",
    "        content = completion.choices[0].message.content\n",
    "        content = content.replace('\\n', '').replace(' ', '')\n",
    "        content_cleaned = re.sub(r\"[^\\d\\[\\],]\", \"\", content)\n",
    "        content_cleaned = re.sub(r\",\\s*]\", \"]\", content_cleaned)\n",
    "        content_cleaned = re.sub(r\",+\", \",\", content_cleaned)\n",
    "        matches = re.findall(r'\\[([^\\[\\]]*?)\\]', content_cleaned)\n",
    "        result_tmp = []\n",
    "        for match in matches:\n",
    "            match_cleaned = match.strip()\n",
    "            if ',' in match_cleaned:\n",
    "                sublist = [int(num) for num in match_cleaned.split(',')]\n",
    "                result_tmp.append(sublist)\n",
    "            else:\n",
    "                 result_tmp.append([int(num) for num in match_cleaned.split()])\n",
    "        for row_slice in result_tmp:\n",
    "            result_sliced.append(row_slice)\n",
    "    parent = list(range(len(result_sliced)))\n",
    "    array_new = find_simi_nex(result_sliced,data_list,parent,ini_simi,the_max_nex)\n",
    "    return array_new,api_call_time,use_time,use_token , seperate_input_token , seperate_output_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_back(two_d_array, three_d_array):\n",
    "    \n",
    "    num_to_row = {}\n",
    "\n",
    "    \n",
    "    for matrix in three_d_array:\n",
    "        for row in matrix:\n",
    "            for number in row:\n",
    "                if number not in num_to_row:\n",
    "                    num_to_row[number] = row\n",
    "\n",
    " \n",
    "    for i, row in enumerate(two_d_array):\n",
    "        new_row = []\n",
    "        for number in row:\n",
    "            if number in num_to_row:\n",
    "                new_row.extend(num_to_row[number])\n",
    "   \n",
    "        two_d_array[i] = list(dict.fromkeys(new_row))\n",
    "\n",
    "    return two_d_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cal_total_simi_vector(data_file_path,model_file):\n",
    "    model = SentenceTransformer(model_file)\n",
    "    def combine_attributes(row):\n",
    "        return ' '.join(str(value) for value in row[1:])  \n",
    "    data = pd.read_csv(data_file_path,encoding=\"MacRoman\")\n",
    "    data['combined_text'] = data.apply(combine_attributes, axis=1)\n",
    "    vectors = data['combined_text'].apply(lambda text: model.encode(text)).tolist()\n",
    "    simi_matrix = cosine_similarity(vectors)\n",
    "    print(\"calculate similarity matrix done\")\n",
    "    return vectors,simi_matrix,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def bipartite_clustering(data, similarity_matrix):\n",
    "\n",
    "    G = nx.Graph()\n",
    "    \n",
    "\n",
    "    for i in range(len(similarity_matrix)):\n",
    "        G.add_node(i)\n",
    "\n",
    "   \n",
    "    for i in range(len(similarity_matrix)):\n",
    "        for j in range(len(similarity_matrix)):\n",
    "            if similarity_matrix[i][j] > 0:\n",
    "                G.add_edge(i, j)\n",
    "\n",
    "\n",
    "    node_partition = nx.bipartite.color(G)\n",
    "\n",
    "\n",
    "    cluster1 = []\n",
    "    cluster2 = []\n",
    "    for i, color in node_partition.items():\n",
    "        if color == 0:\n",
    "            cluster1.append(data[i])\n",
    "        else:\n",
    "            cluster2.append(data[i])\n",
    "\n",
    "    return [cluster1, cluster2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "def read_clusters_from_csv(filename):\n",
    "    clusters = []\n",
    "    with open(filename, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            clusters.append([int(item) for item in row if item]) \n",
    "    return clusters\n",
    "\n",
    "def calculate_purity(true_clusters, predicted_clusters):\n",
    "    total_samples = sum(len(cluster) for cluster in predicted_clusters)\n",
    "    total_correct = 0\n",
    "\n",
    "    for pred_cluster in predicted_clusters:\n",
    "        label_count = Counter()\n",
    "        for sample in pred_cluster:\n",
    "            for true_cluster in true_clusters:\n",
    "                if sample in true_cluster:\n",
    "                    label_count[tuple(true_cluster)] += 1\n",
    "        if label_count:\n",
    "            max_label_count = max(label_count.values())\n",
    "            total_correct += max_label_count\n",
    "\n",
    "    return total_correct / total_samples if total_samples > 0 else 0\n",
    "\n",
    "def calculate_inverse_purity(true_clusters, predicted_clusters):\n",
    "    total_samples = sum(len(cluster) for cluster in true_clusters)\n",
    "    total_correct = 0\n",
    "\n",
    "    for true_cluster in true_clusters:\n",
    "        if true_cluster:\n",
    "            pred_labels = Counter()\n",
    "            for sample in true_cluster:\n",
    "                for pred_cluster in predicted_clusters:\n",
    "                    if sample in pred_cluster:\n",
    "                        pred_labels[tuple(pred_cluster)] += 1\n",
    "            if pred_labels:\n",
    "                max_match = max(pred_labels.values())\n",
    "                total_correct += max_match\n",
    "\n",
    "    return total_correct / total_samples if total_samples > 0 else 0\n",
    "\n",
    "def calculate_fp_measure(true_clusters, predicted_clusters):\n",
    "    purity = calculate_purity(true_clusters, predicted_clusters)\n",
    "    inverse_purity = calculate_inverse_purity(true_clusters, predicted_clusters)\n",
    "\n",
    "    if purity + inverse_purity == 0:\n",
    "        return 0\n",
    "\n",
    "    return 2 * (purity * inverse_purity) / (purity + inverse_purity)\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def convert_to_labels(clusters, n_samples):\n",
    "\n",
    "    labels = [-1] * n_samples \n",
    "    for cluster_id, cluster in enumerate(clusters):\n",
    "        for sample in cluster:\n",
    "            labels[sample] = cluster_id\n",
    "    return labels\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "def calculate_ari(true_clusters, predicted_clusters):\n",
    "\n",
    "    all_samples = set(sample for cluster in true_clusters for sample in cluster) | \\\n",
    "    set(sample for cluster in predicted_clusters for sample in cluster)\n",
    "    n_samples = max(all_samples) + 1 \n",
    "\n",
    "  \n",
    "    true_labels = convert_to_labels(true_clusters, n_samples)\n",
    "    predicted_labels = convert_to_labels(predicted_clusters, n_samples)\n",
    "\n",
    "   \n",
    "    ari = adjusted_rand_score(true_labels, predicted_labels)\n",
    "    return ari\n",
    "\n",
    "def calculate_accuracy(original_list, predicted_list):\n",
    " \n",
    "    original_mapping = {}\n",
    "    for sublist in original_list:\n",
    "        group = tuple(sublist)  \n",
    "        for item in sublist:\n",
    "            original_mapping[item] = group\n",
    "\n",
    " \n",
    "    total_predicted_count = sum(len(sublist) for sublist in predicted_list)\n",
    "\n",
    " \n",
    "    correct_count = 0\n",
    "\n",
    " \n",
    "    checked_items = set()\n",
    "\n",
    " \n",
    "    for sublist in predicted_list:\n",
    "        original_groups = set(original_mapping.get(item, None) for item in sublist)\n",
    "        if len(original_groups) == 1 and None not in original_groups:\n",
    " \n",
    "            correct_count += len(sublist)\n",
    "        else:\n",
    " \n",
    "            for item in sublist:\n",
    "                if original_mapping.get(item, None) in original_groups:\n",
    "                    if item not in checked_items: \n",
    "                        correct_count += 1\n",
    "                checked_items.add(item)\n",
    "\n",
    "\n",
    "    for sublist in predicted_list:\n",
    "        original_groups = set(original_mapping.get(item, None) for item in sublist)\n",
    "        if len(original_groups) == 1 and None not in original_groups:\n",
    " \n",
    "            original_group = original_groups.pop()\n",
    "            if len(sublist) < len(original_group):\n",
    "        \n",
    "                correct_count -= len(sublist)\n",
    "\n",
    "    all_original_items = set(item for sublist in original_list for item in sublist)\n",
    "    all_predicted_items = set(item for sublist in predicted_list for item in sublist)\n",
    "    missed_items = all_original_items - all_predicted_items\n",
    "\n",
    "    correct_count += len(missed_items)\n",
    "\n",
    "    predicted_item_counts = {}\n",
    "    for sublist in predicted_list:\n",
    "        for item in sublist:\n",
    "            if item in predicted_item_counts:\n",
    "                predicted_item_counts[item] += 1\n",
    "            else:\n",
    "                predicted_item_counts[item] = 1\n",
    "\n",
    "    for item, count in predicted_item_counts.items():\n",
    "        if count > 1:\n",
    "            correct_count -= (count - 1)  \n",
    "\n",
    "    for sublist in predicted_list:\n",
    "        for item in sublist:\n",
    "            if item not in all_original_items:\n",
    "                correct_count -= 1  \n",
    "\n",
    "    accuracy = correct_count / total_predicted_count if total_predicted_count > 0 else 0.0\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate(vectors,simi_matrix,merge_clusters_pre,data_file_path,the_max_nex):\n",
    "    api_call_time_all = 0\n",
    "    sperate_time = 0\n",
    "    sperate_token = 0\n",
    "    seperate_input = 0\n",
    "    seperate_output = 0\n",
    "    sperate_result = []\n",
    "    for id_list in merge_clusters_pre:\n",
    "        print(id_list)\n",
    "        text_data = get_data(id_list, data_file_path)\n",
    "        vectorized_data = vectorize_data(text_data)\n",
    "        n_clusters = elbow_method(vectorized_data) \n",
    "        labels = kmeans_clustering(vectorized_data, n_clusters)\n",
    "        clusters_labels = format_output(id_list, labels)\n",
    "        prompt_id = dynamic_sampling(clusters_labels)\n",
    "        classified_results, execute_time , use_number , total_tokens = process_sampled_ids(data_file_path, prompt_id)\n",
    "        sperate_time+=execute_time\n",
    "        api_call_time_all+=use_number\n",
    "        sperate_token+=total_tokens\n",
    "\n",
    "        result_for_found = the_most_importent_one(vectors,classified_results) \n",
    "        target_list = traverse_ids_to_2d(result_for_found, simi_matrix, max_length=10, batch_size=10) \n",
    "        # target_list = three_d_lists[0]\n",
    "        llm_tmp = []\n",
    "        for row_slice in target_list:\n",
    "            print(row_slice)\n",
    "            array_new,api_call_time,use_time,use_token , seperate_input_token , seperate_output_token = llm_seperate(row_slice,data_file_path,simi_matrix,the_max_nex)\n",
    "            api_call_time_all +=api_call_time\n",
    "            sperate_time += use_time\n",
    "            sperate_token += use_token\n",
    "            seperate_input += seperate_input_token\n",
    "            seperate_output += seperate_output_token\n",
    "            llm_tmp = llm_tmp + array_new\n",
    "        find_back_matrix = find_back(llm_tmp,classified_results)\n",
    "        sperate_result += find_back_matrix\n",
    "    print(\"seperate done\")\n",
    "    return sperate_result, api_call_time_all ,sperate_time, sperate_token, seperate_input , seperate_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use canopy block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "execute_jaccard_block_time (s) : 0.6442053318023682\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = './dataset/cora/'\n",
    "data_file_path = file_path+'cora.csv'\n",
    "gt_path = file_path+'gt.csv'\n",
    "cleaned_data = read_csv_and_clean_with_ids(data_file_path)\n",
    "inverted_index = build_inverted_index(cleaned_data)\n",
    "ini_simi, record_ids = calculate_similarity_matrix(cleaned_data, inverted_index)\n",
    "T1 = 0.27\n",
    "T2 = 0.1\n",
    "block_time = time.time()\n",
    "canopies_block = canopy_clustering(ini_simi,  T1, T2)\n",
    "execute_block_time = time.time() -  block_time\n",
    "print(\"done\")\n",
    "print(f\"execute_jaccard_block_time (s) : {execute_block_time}\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "not_done_any = []\n",
    "merge_done = []\n",
    "for row_line in canopies_block: \n",
    "    if len(row_line)<=5:\n",
    "        not_done_any.append(row_line)\n",
    "    else:\n",
    "        merge_done.append(row_line)\n",
    "print(len(merge_done))\n",
    "seperate_threshold = 0.1\n",
    "\n",
    "sperate_result, api_call_time_all ,sperate_time, sperate_token, seperate_input_token , seperate_output_token = seperate_jac(ini_simi,merge_done,data_file_path,seperate_threshold)\n",
    "\n",
    "for row in not_done_any:\n",
    "    sperate_result.append(row)\n",
    "print(f\"api_call_time_seperate: {api_call_time_all}\")\n",
    "print(f\"time_seperate (s): {sperate_time}\")\n",
    "print(f\"token_seperate : {sperate_token}\")\n",
    "print(f\"token_seperate_input : {seperate_input_token}\")\n",
    "print(f\"token_seperate_output : {seperate_output_token}\")\n",
    "true_clusters = get_ground_truth(gt_path)\n",
    "# true_clusters = read_2d_array_from_file(gt_path)\n",
    "# predicted_clusters = merge_clusters_pre\n",
    "predicted_clusters = sperate_result\n",
    "# predicted_clusters = clusters_block\n",
    "purity = calculate_purity(true_clusters, predicted_clusters)\n",
    "inverse_purity = calculate_inverse_purity(true_clusters, predicted_clusters)\n",
    "fp_measure = calculate_fp_measure(true_clusters, predicted_clusters)\n",
    "acc = calculate_accuracy(true_clusters, predicted_clusters)\n",
    "ari = calculate_ari(true_clusters, predicted_clusters)\n",
    "print(f\"FP-Measure: {fp_measure}\")\n",
    "print(f\"ACC: {acc}\")\n",
    "print(f\"ARI: {ari}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "cfcb51ba6a5898a96229f8cdebac4678b232c747c5fc819b474621f23ba7f45c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
